# streamlit_app.py
# ï¼ï¼ï¼ æ¨¡ç¯„è§£ç­” â€œé¡ä¼¼åº¦ã ã‘â€ ã§æ¡ç‚¹ã™ã‚‹è»½é‡ã‚¢ãƒ—ãƒª ï¼ï¼ï¼
# - TAC/LEC/å¤§åŸ/KEC/AAS ãªã©ã®æ¨¡ç¯„è§£ç­”ï¼ˆæœ€å¤§5ä»¶ï¼‰ã‚’å…¥åŠ›
# - å—é¨“è€…è§£ç­”ã¨ã®é¡ä¼¼åº¦ã‚’è¤‡åˆæŒ‡æ¨™ã§ã‚¹ã‚³ã‚¢åŒ–ï¼ˆ0ã€œ100ï¼‰
# - æŒ‡æ¨™ï¼šé‡å¿ƒã‚³ã‚µã‚¤ãƒ³ / ä¸Šä½2å¹³å‡ / ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰Jaccard / ROUGE-L
# - ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§é‡ã¿ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°ã‚’èª¿æ•´

import math
import re
from collections import Counter
from typing import List, Dict, Tuple
import streamlit as st

# -------------------------
# å‰å‡¦ç†ãƒ»ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã¾ã‚ã‚Š
# -------------------------
def _clean(t: str) -> str:
    t = t.lower()
    t = re.sub(r"[^\w\s]", " ", t, flags=re.UNICODE)  # è¨˜å·é™¤å»
    t = re.sub(r"\s+", " ", t).strip()
    return t

def _tokenize(t: str) -> List[str]:
    return _clean(t).split()

def _build_tfidf_matrix(docs: List[str]) -> Tuple[List[Dict[str, float]], Dict[str, float]]:
    tokenized = [ _tokenize(d) for d in docs ]
    df = Counter()
    for toks in tokenized:
        for w in set(toks):
            df[w] += 1
    N = len(docs)
    idf = { w: math.log( (N + 1) / (df[w] + 1) ) + 1.0 for w in df }  # smooth

    tfidf_docs = []
    for toks in tokenized:
        tf = Counter(toks)
        vec = { w: (tf[w] / max(1, len(toks))) * idf.get(w, 0.0) for w in tf }
        # L2 æ­£è¦åŒ–
        norm = math.sqrt(sum(v*v for v in vec.values())) or 1.0
        vec = { w: v/norm for w, v in vec.items() }
        tfidf_docs.append(vec)
    return tfidf_docs, idf

def _cosine(a: Dict[str,float], b: Dict[str,float]) -> float:
    if not a or not b: return 0.0
    # äº¤å·®ã‚­ãƒ¼ã§ååˆ†ï¼ˆé«˜é€ŸåŒ–ï¼‰
    keys = a.keys() if len(a) < len(b) else b.keys()
    s = sum(a.get(k,0.0)*b.get(k,0.0) for k in keys)
    # æ•°å€¤ã‚†ã‚‰ãé˜²æ­¢
    return max(0.0, min(1.0, s))

def _top_k_keywords(vec: Dict[str,float], k:int=10) -> set:
    return set([w for w,_ in sorted(vec.items(), key=lambda x: x[1], reverse=True)[:k]])

def _jaccard(a:set, b:set) -> float:
    if not a and not b: return 0.0
    inter = len(a & b)
    union = len(a | b) or 1
    return inter/union

def _lcs(a_tokens: List[str], b_tokens: List[str]) -> int:
    # ãƒ¡ãƒ¢ãƒªæ§ãˆã‚2è¡ŒDP
    if not a_tokens or not b_tokens: return 0
    prev = [0]*(len(b_tokens)+1)
    for i in range(1, len(a_tokens)+1):
        cur = [0]*(len(b_tokens)+1)
        ai = a_tokens[i-1]
        for j in range(1, len(b_tokens)+1):
            if ai == b_tokens[j-1]:
                cur[j] = prev[j-1] + 1
            else:
                cur[j] = max(prev[j], cur[j-1])
        prev = cur
    return prev[-1]

def _rouge_l(a: str, b: str) -> float:
    a_tok, b_tok = _tokenize(a), _tokenize(b)
    l = _lcs(a_tok, b_tok)
    prec = l / max(1, len(a_tok))
    rec  = l / max(1, len(b_tok))
    if prec==0 and rec==0: return 0.0
    return (2*prec*rec)/(prec+rec)

# -------------------------
# é¡ä¼¼åº¦åˆæˆã‚¹ã‚³ã‚¢
# -------------------------
def similarity_panel_score(standards: List[str], user_answer: str,
                           w_centroid=0.40, w_top2=0.30, w_jacc=0.15, w_rouge=0.15,
                           k_keywords=12) -> Dict[str, float]:
    """
    standards: æ¨¡ç¯„è§£ç­”ã®ãƒªã‚¹ãƒˆï¼ˆç©ºæ–‡å­—ã¯è‡ªå‹•é™¤å¤–ï¼‰
    user_answer: è©•ä¾¡ã™ã‚‹å—é¨“è€…è§£ç­”
    è¿”ã‚Šå€¤: total/å†…è¨³/å„ç¤¾ã¨ã®å€‹åˆ¥ã‚¹ã‚³ã‚¢
    """
    # ç©ºã®æ¨™æº–ã¯é™¤å¤–
    standards = [s for s in standards if isinstance(s, str) and s.strip()]
    if not standards or not user_answer.strip():
        return {
            "total": 0.0,
            "centroid_sim": 0.0,
            "top2_mean": 0.0,
            "jaccard": 0.0,
            "rougeL": 0.0,
            "per_standard": []
        }

    # ãƒ™ã‚¯ãƒˆãƒ«åŒ–
    docs = standards + [user_answer]
    tfidf_vecs, _ = _build_tfidf_matrix(docs)
    std_vecs = tfidf_vecs[:-1]
    user_vec = tfidf_vecs[-1]

    # 1) é‡å¿ƒã‚³ã‚µã‚¤ãƒ³
    centroid = {}
    for v in std_vecs:
        for k,vv in v.items():
            centroid[k] = centroid.get(k,0.0) + vv
    centroid = {k: vv/len(std_vecs) for k,vv in centroid.items()}
    norm = math.sqrt(sum(vv*vv for vv in centroid.values())) or 1.0
    centroid = {k: vv/norm for k,vv in centroid.items()}
    centroid_sim = _cosine(centroid, user_vec)

    # 2) ä¸Šä½2å¹³å‡
    sims = [_cosine(v, user_vec) for v in std_vecs]
    sims_sorted = sorted(sims, reverse=True)
    top2_mean = sum(sims_sorted[:2]) / max(1, len(sims_sorted[:2]))

    # 3) ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰Jaccardï¼ˆå¹³å‡ï¼‰
    user_kw = _top_k_keywords(user_vec, k_keywords)
    std_kw = [_top_k_keywords(v, k_keywords) for v in std_vecs]
    jaccs = [_jaccard(user_kw, kw) for kw in std_kw]
    jaccard_mean = sum(jaccs)/max(1,len(jaccs))

    # 4) ROUGE-Lï¼ˆå¹³å‡ï¼‰
    rouge_scores = [_rouge_l(user_answer, s) for s in standards]
    rouge_mean = sum(rouge_scores)/max(1,len(rouge_scores))

    # ç·åˆï¼ˆ0â€“1ï¼‰
    w_sum = max(1e-9, (w_centroid + w_top2 + w_jacc + w_rouge))
    total = (w_centroid*centroid_sim + w_top2*top2_mean + w_jacc*jaccard_mean + w_rouge*rouge_mean) / w_sum

    # å„ç¤¾ã¨ã®å€‹åˆ¥
    per = []
    for idx, s in enumerate(standards):
        per.append({
            "index": idx+1,
            "cosine": round(sims[idx], 4),
            "rougeL": round(rouge_scores[idx], 4),
            "jaccard_kw": round(jaccs[idx], 4)
        })

    return {
        "total": round(total, 4),
        "centroid_sim": round(centroid_sim, 4),
        "top2_mean": round(top2_mean, 4),
        "jaccard": round(jaccard_mean, 4),
        "rougeL": round(rouge_mean, 4),
        "per_standard": per
    }

# -------------------------
# Streamlit UI
# -------------------------
st.set_page_config(page_title="äºŒæ¬¡ãƒ»ç­”æ¡ˆ é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢", layout="wide")
st.title("ğŸ“Š äºŒæ¬¡è©¦é¨“ãƒ»ç­”æ¡ˆ é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ï¼ˆæ¨¡ç¯„è§£ç­”ãƒ™ãƒ¼ã‚¹ï¼‰")

with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")
    w_centroid = st.slider("é‡å¿ƒã‚³ã‚µã‚¤ãƒ³ã®é‡ã¿", 0.0, 1.0, 0.40, 0.01)
    w_top2     = st.slider("ä¸Šä½2å¹³å‡ã®é‡ã¿",   0.0, 1.0, 0.30, 0.01)
    w_jacc     = st.slider("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰Jaccardã®é‡ã¿", 0.0, 1.0, 0.15, 0.01)
    w_rouge    = st.slider("ROUGE-Lã®é‡ã¿", 0.0, 1.0, 0.15, 0.01)
    k_keywords = st.slider("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°ï¼ˆJaccardç”¨ï¼‰", 5, 30, 12, 1)
    st.caption("â€» åˆè¨ˆã¯å†…éƒ¨ã§è‡ªå‹•æ­£è¦åŒ–ã—ã¾ã™ï¼ˆåˆè¨ˆâ‰ 1ã§ã‚‚OKï¼‰")

st.markdown("**ä½¿ã„æ–¹**ï¼šæ¨¡ç¯„è§£ç­”ï¼ˆæœ€å¤§5ä»¶ï¼‰ã¨ã€è©•ä¾¡ã—ãŸã„ç­”æ¡ˆã‚’å…¥åŠ›ã—ã€Œæ¡ç‚¹ã™ã‚‹ã€ã€‚TACç­”æ¡ˆãŒ80ç‚¹è¿‘ãå‡ºã‚‹ã‚ˆã†ã«ã€é‡ã¿ã‚„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°ã‚’å¾®èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")

cols = st.columns(5)
with cols[0]:
    tac = st.text_area("TAC", height=180, placeholder="æœ¨è‚²ã‚’å®Ÿè·µã™ã‚‹å ´ã§ã‚ã‚‹ä¿è‚²ãƒ»æ•™è‚²æ–½è¨­ã«ãŠã‘ã‚‹å®Ÿè¨¼å®Ÿé¨“ã«ã‚ˆã‚‹æ–°ãŸãªã‚¢ã‚¤ãƒ‡ã‚¢ç²å¾—ã®æ©Ÿä¼šå‰µå‡ºã€å¤§æ‰‹ECã‚µã‚¤ãƒˆã¸ã®å‡ºåº—ã«ã‚ˆã‚‹è²©å£²ãƒãƒ£ãƒãƒ«ã®æ‹¡å¤§ã€ç¤¾é•·ã®å­æ¯ã®çµŒå–¶å­¦ã®çŸ¥è­˜ã‚„Xäº‹æ¥­ã§ã®çµŒé¨“ã«ã‚ˆã‚‹ã€SNSã‚’æ´»ç”¨ã—ãŸæƒ…å ±ç™ºä¿¡ã‚„å­è‚²ã¦ã‚¤ãƒ™ãƒ³ãƒˆã¸ã®å‡ºå±•ãªã©ã®ç©æ¥µçš„ãªä¼ç”»ãƒ»å®Ÿè¡Œãªã©ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå±¤ã‚’æ„è­˜ã—ãŸå†…å®¹ã‚’å®Ÿæ–½ã—ãŸã€‚")
with cols[1]:
    lec = st.text_area("LEC", height=180, placeholder="å–çµ„ã¯ã€ç›´å–¶åº—ã‚„ã‚¢ãƒ³ãƒ†ãƒŠã‚·ãƒ§ãƒƒãƒ—ã€ã‚¤ãƒ™ãƒ³ãƒˆã‚„ä¿è‚²ãƒ»æ•™è‚²æ–½è¨­ã€å¤§æ‰‹ECã‚µã‚¤ãƒˆã‚„SNSã€ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—ã§ã‚ã‚‹ã€‚å·¥å¤«ã¯ã€å…¬å…±å›£ä½“ã¨ã®è‰¯å¥½ãªé–¢ä¿‚ã‚„ç¤¾é•·ã®å­æ¯ã®Xäº‹æ¥­ã§ã®çµŒé¨“ã‚’æ´»ã‹ã—ã€PRã‚„æƒ…å ±ç™ºä¿¡ã‚’å¼·åŒ–ã—ãŸã€‚ã¾ãŸã€é¡§å®¢ãŒè£½å“ã«è§¦ã‚Œã‚‹æ©Ÿä¼šã‚„å­¦ç”Ÿã¨ã®å…±åŒç ”ç©¶æ©Ÿæ¢°ã‚’ä½œã‚Šã€å¸‚å ´ã®æˆé•·å¯èƒ½æ€§ã‚„æ–°ãŸãªã‚¢ã‚¤ãƒ‡ã‚¢ã‚’æ¢ç´¢ã—ãŸã€‚")
with cols[2]:
    ohara = st.text_area("å¤§åŸ", height=180, placeholder="è¡Œã£ãŸå–ã‚Šçµ„ã¿ã¯ã€å¾“æ¥ã®ãƒ«ãƒ¼ãƒˆã¨ã¯ç•°ãªã‚‹æ–°ãŸãªè²©å£²ãƒãƒ£ãƒãƒ«ã®æ§‹ç¯‰ã§ã‚ã‚‹ã€‚20ä»£ã‹ã‚‰40ä»£ã®æ•™è‚²ç†±å¿ƒãªå­è‚²ã¦å®¶åº­ã¨ã®æ¥ç‚¹ã‚’ã‚‚ã¤ãŸã‚ã«ã€å¤§æ‰‹ECã‚µã‚¤ãƒˆã¸å‡ºåº—ã—ãŸã€‚å·¥å¤«ã¯ã€â‘ SNSã‚’æ´»ç”¨ã—ãŸæƒ…å ±ç™ºä¿¡ã‚„å­è‚²ã¦ã‚¤ãƒ™ãƒ³ãƒˆã¸ã®å‡ºå±•ç­‰ã«ã‚ˆã‚‹èªçŸ¥åº¦å‘ä¸Šã€â‘¡åœ°å…ƒã®å¤§å­¦ã¨ã®æ•™è‚²é€£æºã®æ¨é€²ã«ã‚ˆã‚‹è£½å“é–‹ç™ºã‚µã‚¤ã‚¯ãƒ«ã®jåŠ é€Ÿã†ã€ç­‰ã§ã‚ã‚‹ã€‚")
with cols[3]:
    kec = st.text_area("KEC", height=180, placeholder="å–çµ„ã¿ã¯ã€è‡ªç¤¾ä½µè¨­ã®ç›´å–¶åº—ã‚„çœŒã®ã‚¢ãƒ³ãƒ†ãƒŠã‚·ãƒ§ãƒƒãƒ—ãƒ»å¤§æ‰‹ECã‚µã‚¤ãƒˆã§ã®è²©å£²ã®ä»–ã€SNSã‚’æ´»ç”¨ã—ãŸæƒ…å ±ç™ºä¿¡ã‚„å­è‚²ã¦ã‚¤ãƒ™ãƒ³ãƒˆã¸ã®å‡ºå±•ã§ã‚ã‚‹ã€‚å·¥å¤«ã¯ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’æ•™è‚²ç†±å¿ƒãªå®¶åº­ã¨ã—ã€åœ°å…ƒã®çœŒã‚„å¤§å­¦ã¨ã®é–¢ä¿‚ã‚’ç”Ÿã‹ã—ãŸä¿è‚²ãƒ»æ•™è‚²æ–½è¨­ã«ãŠã„ã¦å­ä¾›ãŸã¡ãŒæ—¥å¸¸çš„ã«Aç¤¾è£½å“ã«è§¦ã‚Œã‚‹æ©Ÿä¼šã‚’ä½œã‚Šã€æ–°è£½å“ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã«ç¹‹ã’ãŸã€‚")
with cols[4]:
    aas = st.text_area("AAS/è‡ªä½œï¼ˆä»»æ„ï¼‰", height=180, placeholder="AASã‚„è‡ªä½œã®æ¨™æº–è§£ç­”ã‚’è²¼ã‚Šä»˜ã‘")

user_answer = st.text_area("ğŸ“ è©•ä¾¡ã—ãŸã„ç­”æ¡ˆï¼ˆå¿…é ˆï¼‰", height=200, placeholder="ã‚ãªãŸã®ç­”æ¡ˆã‚’ã“ã“ã«è²¼ã‚Šä»˜ã‘")

do_score = st.button("ğŸ¯ æ¡ç‚¹ã™ã‚‹", type="primary")

if do_score:
    standards = [tac, lec, ohara, kec, aas]
    scores = similarity_panel_score(
        standards, user_answer,
        w_centroid=w_centroid, w_top2=w_top2, w_jacc=w_jacc, w_rouge=w_rouge,
        k_keywords=k_keywords
    )

    if not user_answer.strip():
        st.warning("è©•ä¾¡å¯¾è±¡ã®ç­”æ¡ˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    elif not any(s.strip() for s in standards):
        st.warning("æ¨¡ç¯„è§£ç­”ã‚’1ã¤ä»¥ä¸Šå…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        st.subheader("çµæœ")
        total_pct = int(round(scores["total"]*100))
        c1, c2, c3 = st.columns([1,1,2])
        with c1:
            st.metric("ç·åˆã‚¹ã‚³ã‚¢", f"{total_pct} / 100")
        with c2:
            st.write("**å†…è¨³ï¼ˆ0ã€œ1ï¼‰**")
            st.write({
                "é‡å¿ƒã‚³ã‚µã‚¤ãƒ³": scores["centroid_sim"],
                "ä¸Šä½2å¹³å‡": scores["top2_mean"],
                "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰Jaccard": scores["jaccard"],
                "ROUGE-L": scores["rougeL"],
            })
        with c3:
            st.write("**å„æ¨¡ç¯„è§£ç­”ã¨ã®å€‹åˆ¥ã‚¹ã‚³ã‚¢ï¼ˆ0ã€œ1ï¼‰**")
            if scores["per_standard"]:
                st.table(scores["per_standard"])
            else:
                st.info("æ¨¡ç¯„è§£ç­”ãŒæœªå…¥åŠ›ã§ã™ã€‚")

        st.divider()
        st.caption("ãƒ’ãƒ³ãƒˆï¼šTACç­”æ¡ˆã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ç­”æ¡ˆã«å…¥ã‚Œã¦å®Ÿè¡Œ â‡’ å†…è¨³ã‚’è¦‹ãªãŒã‚‰é‡ã¿ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°ã‚’å¾®èª¿æ•´ã™ã‚‹ã¨ã€åŸºæº–åˆã‚ã›ãŒç´ æ—©ãã§ãã¾ã™ã€‚")
